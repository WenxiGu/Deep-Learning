{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "540c8652",
      "metadata": {
        "id": "540c8652"
      },
      "source": [
        "It is highly recommended to use a powerful **GPU**, you can use it for free uploading this notebook to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb).\n",
        "<table align=\"center\">\n",
        " <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/ezponda/intro_deep_learning/blob/main/class/NLP/semantic_search_QA_clustering.ipynb\">\n",
        "        <img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/ezponda/intro_deep_learning/blob/main/class/NLP/semantic_search_QA_clustering.ipynb\">\n",
        "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\"  width=\"50\" height=\"50\" style=\"padding-bottom:5px;\" />View Source on GitHub</a></td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e220629",
      "metadata": {
        "id": "7e220629"
      },
      "source": [
        "# Semantic search & QA\n",
        "\n",
        "In this notebook, we'll introduce semantic search and question-answering using [`sentence-transformers`](https://www.sbert.net/), a Python library for state-of-the-art sentence, text and image embeddings. These embeddings are useful for semantic similarity tasks, such as information retrieval and question-answering systems."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#embedding\n",
        "\n",
        "æŠŠä¸€å¥è¯â€œç¼–ç æˆå‘é‡åµŒå…¥â€ï¼Œå¯ä»¥è¿™æ ·ç†è§£ï¼š\n",
        "\n",
        "ç›´è§‰ç‰ˆ\n",
        "\n",
        "æŠŠæ–‡æœ¬ â†’ æ•°å­—åæ ‡ï¼šæ¨¡å‹æŠŠä¸€å¥è¯æ˜ å°„åˆ°ä¸€ä¸ªé«˜ç»´åæ ‡ç‚¹ï¼Œæ¯”å¦‚ 384 ç»´æˆ– 768 ç»´çš„å‘é‡ vã€‚\n",
        "\n",
        "è¯­ä¹‰è¿‘çš„ï¼åæ ‡è¿‘ï¼šæ„æ€ç›¸è¿‘çš„å¥å­è½åœ¨ç©ºé—´é‡Œå½¼æ­¤æ›´è¿‘ï¼›ä¸ç›¸å…³çš„è½å¾—è¿œã€‚\n",
        "\n",
        "ç”¨ç›¸ä¼¼åº¦ï¼ˆå¸¸ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰é‡åŒ–â€œè¿‘ä¸è¿‘â€ã€‚\n",
        "\n",
        "ä¸ºä»€ä¹ˆè¦è¿™æ ·åš\n",
        "\n",
        "è®¡ç®—æœºä¸æ‡‚è¯ä¹‰ï¼Œä½†ä¼šç®—å‘é‡ã€‚æœ‰äº†å‘é‡ï¼Œå°±èƒ½åšï¼š\n",
        "\n",
        "è¯­ä¹‰æ£€ç´¢ï¼ˆæ‰¾æœ€ç›¸ä¼¼çš„æ®µè½ï¼‰\n",
        "\n",
        "å»é‡/èšç±»ï¼ˆæ‰¾ç›¸ä¼¼é—®é¢˜ï¼‰\n",
        "\n",
        "è¯­ä¹‰åŒ¹é…ã€æ¨èã€æ’åº\n",
        "\n",
        "æ€ä¹ˆå¾—åˆ°è¿™ä¸ªå‘é‡\n",
        "\n",
        "ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ sentence-transformers çš„ bi-encoderï¼‰ã€‚\n",
        "\n",
        "è®­ç»ƒç›®æ ‡å¸¸è§æ˜¯å¯¹æ¯”å­¦ä¹ ï¼šè®©â€œæ­£æ ·æœ¬å¯¹â€ï¼ˆåŒä¹‰/é…å¯¹çš„å¥å­ï¼‰æ›´è¿‘ï¼Œâ€œè´Ÿæ ·æœ¬å¯¹â€æ›´è¿œã€‚\n",
        "\n",
        "æ¨ç†æ—¶ï¼šemb = model.encode(text) â†’ å¾—åˆ°ä¸€ä¸ªå®šé•¿å‘é‡ã€‚\n",
        "\n",
        "å‘é‡é‡Œçš„æ¯ä¸€ç»´è¡¨ç¤ºä»€ä¹ˆï¼Ÿ\n",
        "\n",
        "å®ƒä»¬æ˜¯åˆ†å¸ƒå¼ç‰¹å¾ï¼šä¸æ˜¯â€œç¬¬ 1 ç»´=æƒ…æ„Ÿï¼Œç¬¬ 2 ç»´=åœ°ç‚¹â€è¿™ç§å¯è¯»æ ‡ç­¾ï¼Œè€Œæ˜¯æ¨¡å‹è‡ªåŠ¨å­¦åˆ°çš„æŠ½è±¡è¯­ä¹‰è½´ã€‚å•ç»´å¾ˆéš¾è§£é‡Šï¼Œä½†æ•´ä½“æ–¹å‘å¯ç”¨æ¥æ¯”ç›¸ä¼¼åº¦ã€‚\n",
        "\n",
        "ç›¸ä¼¼åº¦æ€ä¹ˆé‡\n",
        "\n",
        "å¸¸ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ¯”è¾ƒæ–¹å‘ï¼Œä¸çœ‹é•¿åº¦ï¼‰ï¼š\n",
        "\n",
        "cos\n",
        "(\n",
        "ğ‘¢\n",
        ",\n",
        "ğ‘£\n",
        ")\n",
        "=\n",
        "ğ‘¢\n",
        "â‹…\n",
        "ğ‘£\n",
        "âˆ¥\n",
        "ğ‘¢\n",
        "âˆ¥\n",
        "âˆ¥\n",
        "ğ‘£\n",
        "âˆ¥\n",
        "cos(u,v)=\n",
        "âˆ¥uâˆ¥âˆ¥vâˆ¥\n",
        "uâ‹…v\n",
        "\tâ€‹\n",
        "\n",
        "\n",
        "å®è·µé‡Œå¸¸æŠŠå‘é‡å½’ä¸€åŒ–ï¼ˆå•ä½é•¿åº¦ï¼‰ååšæ£€ç´¢ï¼Œæ›´ç¨³ã€‚\n",
        "\n",
        "ä¸€ä¸ªç©å…·ä¾‹å­ï¼ˆæƒ³è±¡ 3 ç»´ï¼‰\n",
        "\n",
        "â€œI love catsâ€ â†’\n",
        "ğ‘£\n",
        "1\n",
        "=\n",
        "(\n",
        "0.6\n",
        ",\n",
        "\n",
        "0.7\n",
        ",\n",
        "\n",
        "0.2\n",
        ")\n",
        "v\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "=(0.6,0.7,0.2)\n",
        "\n",
        "â€œI like kittensâ€ â†’\n",
        "ğ‘£\n",
        "2\n",
        "=\n",
        "(\n",
        "0.58\n",
        ",\n",
        "\n",
        "0.72\n",
        ",\n",
        "\n",
        "0.18\n",
        ")\n",
        "v\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "=(0.58,0.72,0.18) ï¼ˆä¸\n",
        "ğ‘£\n",
        "1\n",
        "v\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        " å¾ˆè¿‘ â†’ è¯­ä¹‰ç›¸ä¼¼ï¼‰\n",
        "\n",
        "â€œThe stock market fellâ€ â†’\n",
        "ğ‘£\n",
        "3\n",
        "=\n",
        "(\n",
        "âˆ’\n",
        "0.1\n",
        ",\n",
        "\n",
        "âˆ’\n",
        "0.3\n",
        ",\n",
        "\n",
        "0.9\n",
        ")\n",
        "v\n",
        "3\n",
        "\tâ€‹\n",
        "\n",
        "=(âˆ’0.1,âˆ’0.3,0.9) ï¼ˆæ–¹å‘å·®å¾ˆå¤š â†’ ä¸ç›¸ä¼¼ï¼‰\n",
        "\n",
        "å’Œâ€œä¸‰ç»´/é«˜ç»´â€å…³ç³»\n",
        "\n",
        "è¿™æ˜¯ä¸€ä¸ªé«˜ç»´ç©ºé—´ï¼ˆå‡ ååˆ°ä¸Šåƒç»´ï¼‰ã€‚ç»´åº¦=å®šä½ä¸€ä¸ªç‚¹æ‰€éœ€çš„æœ€å°‘æ•°å­—ä¸ªæ•°ã€‚è¿™é‡Œçš„ç‚¹=ä¸€å¥è¯çš„è¯­ä¹‰ä½ç½®ã€‚\n",
        "\n",
        "å¥½çš„å®è·µ\n",
        "\n",
        "é€‰æ‹©åˆé€‚çš„æ¨¡å‹å°ºå¯¸ï¼ˆå¦‚ all-MiniLM-L6-v2 384 ç»´ï¼Œé€Ÿåº¦å¿«ï¼›æ›´å¤§æ¨¡å‹æ›´å‡†ä½†æ…¢ï¼‰ã€‚\n",
        "\n",
        "æ‰¹é‡æ–‡æœ¬å…ˆç¼–ç å¹¶ç¼“å­˜/ç´¢å¼•ï¼ˆå¦‚ FAISS/Annoy/HNSWï¼‰ï¼ŒæŸ¥è¯¢æ—¶å†è®¡ç®—ç›¸ä¼¼åº¦ã€‚\n",
        "\n",
        "éœ€è¦æ›´å‡†çš„æœ€ç»ˆæ’åºæ—¶ï¼Œç”¨ CrossEncoder å¯¹ top-k é‡æ–°æ‰“åˆ†ã€‚"
      ],
      "metadata": {
        "id": "NN-Xg62B8aSG"
      },
      "id": "NN-Xg62B8aSG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c85446b7",
      "metadata": {
        "id": "c85446b7"
      },
      "outputs": [],
      "source": [
        "# Install the sentence-transformers library\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8fef53ec",
      "metadata": {
        "id": "8fef53ec"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import gzip\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¸€å¥è¯æ€»ç»“ï¼šEmbedding = æŠŠè¯­ä¹‰è£…è¿›å‘é‡ï¼Œè®©â€œæ„ä¹‰çš„è¿œè¿‘â€å˜æˆâ€œå‡ ä½•çš„è¿œè¿‘â€ã€‚"
      ],
      "metadata": {
        "id": "-hTs344u9Hs9"
      },
      "id": "-hTs344u9Hs9"
    },
    {
      "cell_type": "markdown",
      "id": "9ee4a0a8",
      "metadata": {
        "id": "9ee4a0a8"
      },
      "source": [
        "We'll use a pre-trained Sentence Transformer model to generate sentence embeddings. Many pre-trained models are available [here](https://www.sbert.net/docs/pretrained_models.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56081ef1",
      "metadata": {
        "id": "56081ef1"
      },
      "outputs": [],
      "source": [
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7ab530d",
      "metadata": {
        "id": "f7ab530d"
      },
      "source": [
        "For our semantic search and question-answering task, we need a list of documents or paragraphs to search through for relevant information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b473c53a",
      "metadata": {
        "id": "b473c53a"
      },
      "outputs": [],
      "source": [
        "# Sample paragraphs\n",
        "paragraphs = [\n",
        "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\",\n",
        "    \"The Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, in the United States.\",\n",
        "    \"The Great Wall of China is a series of fortifications made of stone, brick, tamped earth, wood, and other materials, generally built along an east-to-west line across the historical northern borders of China.\",\n",
        "    \"The Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy.\",\n",
        "    \"The Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra.\"\n",
        "]\n",
        "\n",
        "paragraphs = np.array(paragraphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2ad2ca34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ad2ca34",
        "outputId": "e8c1ef9d-1e04-4143-a3a0-fd693d0101d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 384)\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings for paragraphs\n",
        "corpus_embeddings = model.encode(paragraphs)\n",
        "print(corpus_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿™ä¸¤æ ¼ä»£ç åœ¨åšä¸€ä»¶äº‹ï¼šæŠŠ 5 æ®µç¤ºä¾‹æ–‡æœ¬ç¼–ç æˆå‘é‡åµŒå…¥ï¼Œä¸ºåç»­è¯­ä¹‰æ£€ç´¢åšå‡†å¤‡ã€‚\n",
        "\n",
        "paragraphs = [...]ï¼šå‡†å¤‡ 5 æ®µå…³äºåœ°æ ‡çš„è‹±æ–‡æ®µè½ï¼ˆè¯­æ–™åº“ corpusï¼‰ã€‚\n",
        "\n",
        "paragraphs = np.array(paragraphs)ï¼šè½¬æˆ numpy æ•°ç»„ï¼Œä¾¿äºæ‰¹é‡ç¼–ç ã€‚\n",
        "\n",
        "corpus_embeddings = model.encode(paragraphs)ï¼šç”¨ä½ ä¸Šé¢åŠ è½½çš„ SentenceTransformer æ¨¡å‹ï¼ŒæŠŠæ¯æ®µæ–‡æœ¬ â†’ ä¸€ä¸ªå®šé•¿å‘é‡ã€‚\n",
        "\n",
        "print(corpus_embeddings.shape) è¾“å‡º (5, 384)ï¼šè¡¨ç¤ºå…±æœ‰ 5 ä¸ªæ®µè½ï¼Œæ¯ä¸ªæ®µè½è¢«ç¼–ç æˆ 384 ç»´å‘é‡ï¼ˆä½ ç”¨çš„æ¨¡å‹çš„é»˜è®¤ç»´åº¦ï¼‰ã€‚\n",
        "\n",
        "è¿™äº›åµŒå…¥ç¨åä¼šç”¨ä½™å¼¦ç›¸ä¼¼åº¦ä¸æŸ¥è¯¢å‘é‡æ¯”å¯¹ï¼Œæ‰¾å‡ºè¯­ä¹‰æœ€ç›¸è¿‘çš„æ®µè½ã€‚"
      ],
      "metadata": {
        "id": "OeDpKZpN_cXW"
      },
      "id": "OeDpKZpN_cXW"
    },
    {
      "cell_type": "markdown",
      "id": "87a5756f",
      "metadata": {
        "id": "87a5756f"
      },
      "source": [
        "Now, let's define a function to perform semantic search, given a query and a list of paragraph embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ad6a9592",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad6a9592",
        "outputId": "80cd68df-887f-46c2-f9ef-12b8cbf9ce18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input query: Where is the Colosseum\n",
            "\n",
            "0.801\tThe Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy.\n",
            "0.226\tThe Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra.\n"
          ]
        }
      ],
      "source": [
        "def semantic_search(query, model, corpus_embeddings, paragraphs, top_k=2):\n",
        "    query_embedding = model.encode([query])[0] # # 1) æŠŠæŸ¥è¯¢å¥å­ç¼–ç æˆå‘é‡\n",
        "    similarities = cosine_similarity([query_embedding], corpus_embeddings)[0] # # 2) è®¡ç®—æŸ¥è¯¢ä¸åº“ä¸­æ¯ä¸ªæ®µè½çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè¿”å›å½¢çŠ¶ 1Ã—Nï¼Œå–ç¬¬0è¡Œå˜æˆ(N,)ï¼‰\n",
        "    indexes = np.argpartition(similarities, -top_k)[-top_k:] #3) å…ˆç”¨ argpartition å–å‡ºç›¸ä¼¼åº¦å‰ top_k åï¼ˆO(N) è¿‘ä¼¼çº¿æ€§ï¼Œå¿«ï¼‰\n",
        "    indexes = indexes[np.argsort(-similarities[indexes])] # 4) å†åœ¨è¿™ top_k é‡ŒæŒ‰åˆ†æ•°ä»å¤§åˆ°å°ç²¾ç¡®æ’åº\n",
        "    print(f\"Input query: {query}\")\n",
        "    print()\n",
        "    for text, sim in zip(list(paragraphs[indexes]), similarities[indexes].tolist()):\n",
        "        print(f\"{sim:.3f}\\t{text}\")\n",
        "\n",
        "\n",
        "semantic_search('Where is the Colosseum', model, corpus_embeddings, paragraphs, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6df5645",
      "metadata": {
        "id": "b6df5645"
      },
      "source": [
        "## Multilingual models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "93b0cded",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93b0cded",
        "outputId": "4fd82217-be5d-4384-9419-4447fc070be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input query: Â¿DÃ³nde estÃ¡ el Coliseo?\n",
            "\n",
            "0.086\tThe Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, in the United States.\n",
            "0.067\tThe Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra.\n"
          ]
        }
      ],
      "source": [
        "# lets try in other languages\n",
        "semantic_search('Â¿DÃ³nde estÃ¡ el Coliseo?', model, corpus_embeddings, paragraphs, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d55f723",
      "metadata": {
        "id": "5d55f723"
      },
      "source": [
        "We have multilinguals models available [here](https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea38223",
      "metadata": {
        "id": "5ea38223"
      },
      "outputs": [],
      "source": [
        "#Â we can use multilingual models\n",
        "model_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "multi_model = SentenceTransformer(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6d7e03d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d7e03d2",
        "outputId": "38b5e36c-bf44-492d-b15f-9666487314b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 384)\n"
          ]
        }
      ],
      "source": [
        "multi_corpus_embeddings = multi_model.encode(paragraphs)\n",
        "print(multi_corpus_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a6d89f20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6d89f20",
        "outputId": "4e2e482b-e451-4b48-84d7-965b57f65abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input query: Â¿DÃ³nde estÃ¡ el Coliseo?\n",
            "\n",
            "0.439\tThe Colosseum, also known as the Flavian Amphitheatre, is an oval amphitheatre in the centre of the city of Rome, Italy.\n",
            "0.299\tThe Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, in the United States.\n"
          ]
        }
      ],
      "source": [
        "semantic_search('Â¿DÃ³nde estÃ¡ el Coliseo?', multi_model, multi_corpus_embeddings, paragraphs, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08858a4b",
      "metadata": {
        "id": "08858a4b"
      },
      "source": [
        "## Wikipedia semantic search\n",
        "\n",
        "As dataset, we use Simple English Wikipedia. Compared to the full English wikipedia, it has only\n",
        "about 170k articles. We split these articles into paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15aef3ce",
      "metadata": {
        "id": "15aef3ce"
      },
      "outputs": [],
      "source": [
        "wikipedia_filepath = 'data/simplewiki-2020-11-01.jsonl.gz'\n",
        "\n",
        "if not os.path.exists(wikipedia_filepath):\n",
        "    util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)\n",
        "\n",
        "passages = []\n",
        "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
        "    for line in fIn:\n",
        "        data = json.loads(line.strip())\n",
        "        for paragraph in data['paragraphs']:\n",
        "            # We encode the passages as [title, text]\n",
        "            passages.append(data['title']+':  '+ paragraph)\n",
        "\n",
        "# If you like, you can also limit the number of passages you want to use\n",
        "print(\"Passages:\", len(passages))\n",
        "print(passages[0])\n",
        "print(passages[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fa111507",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa111507",
        "outputId": "c5225321-407b-4db4-9b58-0910c336b980"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "reduced_passages = np.array(passages[:5000])\n",
        "reduced_passages.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4cc331",
      "metadata": {
        "id": "5a4cc331"
      },
      "outputs": [],
      "source": [
        "corpus_embeddings = model.encode(reduced_passages, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c7d1260d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7d1260d",
        "outputId": "dcd03442-c84d-4549-b040-0724f874d234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input query: Best american actor\n",
            "\n",
            "0.539\tAaron Kwok:  Aaron won the Best Actor Award again at the forty-third Golden Horse Awards on 24 November 2006 for his role in the movie \"After This Our Exile\". He became the second actor in the history of the Golden Horse Awards to win the Best Actor Award year after year. Jackie Chan first achieved this back in the 1990s.\n",
            "0.425\tJames L. Brooks:  He is best known for creating American television programs such as \"The Mary Tyler Moore Show\", \"The Simpsons\", \"Rhoda\" and \"Taxi\". His best-known movie is \"Terms of Endearment\", for which he received three Academy Awards in 1984.\n"
          ]
        }
      ],
      "source": [
        "semantic_search('Best american actor', model, corpus_embeddings, reduced_passages, top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fbad4f4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbad4f4a",
        "outputId": "947c2f59-fccd-4dd5-d8d6-3e59aeae5fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input query: Number countries Europe\n",
            "\n",
            "0.502\tEuropean Union member state:  A European Union member state is any one of the twenty-seven countries that have joined the European Union (EU) since it was found in 1958 as the European Economic Community (EEC). From an original membership of six states, there have been five successive enlargements. The largest happened on 1 May 2004, when ten member states joined.\n",
            "0.465\tEuropean Space Agency:  The member countries of ESA are Austria, Belgium, Czech Republic, Denmark, Finland, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Norway, Portugal, Spain, Sweden, Switzerland and the United Kingdom.\n"
          ]
        }
      ],
      "source": [
        "semantic_search('Number countries Europe', model, corpus_embeddings, reduced_passages, top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327dce75",
      "metadata": {
        "id": "327dce75"
      },
      "source": [
        "### Question1: Load a different pre-trained Sentence Transformer model and compare its performance to the last model on the same set of paragraphs and queries. Which model performs better?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a different pre-trained model, generate embeddings, and test with the same queries\n",
        "model_name = 'all-mpnet-base-v2'\n",
        "new_model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "id": "2lpXMTksB1vw"
      },
      "id": "2lpXMTksB1vw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8eb3ca16",
      "metadata": {
        "id": "8eb3ca16"
      },
      "source": [
        "## Question 2: Find text duplicates\n",
        "\n",
        "Try to find duplicate or near-duplicate texts in a given corpus based on their semantic similarity using sentence-transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2e63124d",
      "metadata": {
        "id": "2e63124d"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The quick brown fox leaps over the lazy dog.\",\n",
        "    \"The sky is blue, and the grass is green.\",\n",
        "    \"The grass is green, and the sky is blue.\",\n",
        "    \"It's a sunny day today.\",\n",
        "    \"The weather is sunny today.\",\n",
        "    \"She was wearing a beautiful red dress.\",\n",
        "    \"She had on a gorgeous red dress.\",\n",
        "    \"I'm going to the supermarket to buy some groceries.\",\n",
        "    \"I'm heading to the supermarket to purchase some groceries.\",\n",
        "    \"He didn't like the movie because it was too long.\",\n",
        "    \"He disliked the movie as it was too lengthy.\",\n",
        "    \"The train was delayed due to technical issues.\",\n",
        "    \"Technical issues caused the train to be delayed.\",\n",
        "    \"I'll have a cup of coffee with milk and sugar, please.\",\n",
        "    \"Can I get a coffee with milk and sugar, please?\",\n",
        "    \"The conference was very informative and interesting.\",\n",
        "    \"The conference turned out to be interesting and informative.\",\n",
        "    \"He enjoys listening to classical music in his free time.\",\n",
        "    \"In his leisure time, he likes to listen to classical music.\",\n",
        "    \"Please make sure you turn off the lights before leaving.\",\n",
        "    \"Before leaving, ensure that you switch off the lights.\",\n",
        "    \"The boy was delighted with the gift he received.\",\n",
        "    \"Receiving the present made the young lad ecstatic.\",\n",
        "    \"She has a preference for Italian cuisine.\",\n",
        "    \"Her favorite type of food is from Italy.\",\n",
        "    \"The software engineer resolved the issue by modifying the code.\",\n",
        "    \"By altering the programming, the tech expert fixed the problem.\",\n",
        "    \"Due to the inclement weather, the baseball game was postponed.\",\n",
        "    \"The baseball match was rescheduled because of bad weather conditions.\",\n",
        "    \"The house was engulfed in a raging fire.\",\n",
        "    \"Flames rapidly consumed the residence.\",\n",
        "    \"He is constantly browsing the internet for the latest news.\",\n",
        "    \"He frequently scours the web to stay updated on current events.\",\n",
        "    \"The puppy was playing with a toy in the garden.\",\n",
        "    \"In the yard, the young dog was frolicking with its plaything.\",\n",
        "    \"The artist painted a beautiful landscape on the canvas.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3b500687",
      "metadata": {
        "id": "3b500687"
      },
      "outputs": [],
      "source": [
        "# Step 1: Initialize the SentenceTransformer model\n",
        "model = SentenceTransformer('all-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe93930",
      "metadata": {
        "id": "5fe93930"
      },
      "outputs": [],
      "source": [
        "# Step 2: Obtain corpus embeddings\n",
        "embeddings = model.encode(corpus, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "fce3fd65",
      "metadata": {
        "id": "fce3fd65"
      },
      "outputs": [],
      "source": [
        "# Step 3: Calculate similarity and find duplicates\n",
        "\n",
        "# TODO: Define a similarity threshold\n",
        "similarity_threshold = 0.85 #è®¾å®šé˜ˆå€¼ã€‚ä½™å¼¦ç›¸ä¼¼åº¦èŒƒå›´æ˜¯ [-1, 1]ï¼ˆä¸€èˆ¬è¯­ä¹‰åµŒå…¥è½åœ¨ [0, 1]ï¼‰ã€‚åˆ†æ•°é«˜äº 0.85 å°±è®¤ä¸ºâ€œå¾ˆåƒ/å¯èƒ½é‡å¤â€ã€‚\n",
        "\n",
        "# TODO: Iterate over each pair of embeddings in the corpus\n",
        "# Calculate the cosine similarity between the embeddings\n",
        "# If the similarity is above the threshold, add the sentences to the duplicates list\n",
        "duplicates = []\n",
        "\n",
        "for i, emb1 in enumerate(embeddings): #å¤–å±‚å¾ªç¯ï¼šéå†ç¬¬ i ä¸ªå‘é‡ emb1ã€‚embeddings æ˜¯ä½ ä¹‹å‰ç®—å¥½çš„è¯­æ–™åº“åµŒå…¥çŸ©é˜µï¼Œå½¢å¦‚ (N, D)ã€‚\n",
        "    for j, emb2 in enumerate(embeddings[i + 1:]): #å†…å±‚å¾ªç¯ï¼šåªå’Œåé¢çš„å‘é‡æ¯”è¾ƒï¼ˆä» i+1 å¼€å§‹ï¼‰ã€‚\n",
        "        similarity = cosine_similarity([emb1], [emb2])[0][0] #è®¡ç®— ä½™å¼¦ç›¸ä¼¼åº¦ã€‚\n",
        "        if similarity > similarity_threshold:\n",
        "            duplicates.append((corpus[i], corpus[i + j + 1], similarity))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8b17148d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b17148d",
        "outputId": "bf0ccc45-355e-4172-be06-b2660d709678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate sentences:\n",
            "The quick brown fox jumps over the lazy dog. | The quick brown fox leaps over the lazy dog. | Similarity: 0.99\n",
            "\n",
            "The sky is blue, and the grass is green. | The grass is green, and the sky is blue. | Similarity: 0.98\n",
            "\n",
            "It's a sunny day today. | The weather is sunny today. | Similarity: 0.93\n",
            "\n",
            "She was wearing a beautiful red dress. | She had on a gorgeous red dress. | Similarity: 0.98\n",
            "\n",
            "I'm going to the supermarket to buy some groceries. | I'm heading to the supermarket to purchase some groceries. | Similarity: 0.98\n",
            "\n",
            "He didn't like the movie because it was too long. | He disliked the movie as it was too lengthy. | Similarity: 0.94\n",
            "\n",
            "The train was delayed due to technical issues. | Technical issues caused the train to be delayed. | Similarity: 0.98\n",
            "\n",
            "I'll have a cup of coffee with milk and sugar, please. | Can I get a coffee with milk and sugar, please? | Similarity: 0.85\n",
            "\n",
            "The conference was very informative and interesting. | The conference turned out to be interesting and informative. | Similarity: 0.96\n",
            "\n",
            "He enjoys listening to classical music in his free time. | In his leisure time, he likes to listen to classical music. | Similarity: 0.94\n",
            "\n",
            "Please make sure you turn off the lights before leaving. | Before leaving, ensure that you switch off the lights. | Similarity: 0.91\n",
            "\n",
            "She has a preference for Italian cuisine. | Her favorite type of food is from Italy. | Similarity: 0.91\n",
            "\n",
            "Due to the inclement weather, the baseball game was postponed. | The baseball match was rescheduled because of bad weather conditions. | Similarity: 0.85\n",
            "\n",
            "The house was engulfed in a raging fire. | Flames rapidly consumed the residence. | Similarity: 0.89\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Duplicate sentences:\")\n",
        "for sent1, sent2, sim in duplicates:\n",
        "    print(f\"{sent1} | {sent2} | Similarity: {sim:.2f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6dbab82",
      "metadata": {
        "id": "c6dbab82"
      },
      "source": [
        "# Document Clustering\n",
        "\n",
        "K-means clustering is a popular unsupervised machine learning algorithm that groups data points into k clusters based on their similarity. In our case, we want to group documents based on their semantic similarity. The algorithm requires us to specify the number of clusters k in advance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means èšç±»æ˜¯å¸¸è§çš„æ— ç›‘ç£æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå®ƒæ ¹æ®ç›¸ä¼¼åº¦æŠŠæ•°æ®ç‚¹åˆ†æˆ k ä¸ªç°‡ã€‚\n",
        "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æƒ³æ ¹æ®æ–‡æ¡£çš„è¯­ä¹‰ç›¸ä¼¼æ€§æ¥åˆ†ç»„ã€‚\n",
        "è¯¥ç®—æ³•éœ€è¦æˆ‘ä»¬äº‹å…ˆæŒ‡å®šç°‡çš„æ•°é‡ kã€‚"
      ],
      "metadata": {
        "id": "EGxjiV0mFPY1"
      },
      "id": "EGxjiV0mFPY1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç”¨ K-means ç»™æ–‡æ¡£åˆ†ç¾¤ã€‚\n",
        "\n",
        "æ ¸å¿ƒæ€è·¯\n",
        "\n",
        "å…ˆæŠŠæ¯ä¸ªæ–‡æ¡£ç¼–ç æˆå‘é‡åµŒå…¥ï¼ˆsentence-transformersï¼‰ã€‚\n",
        "\n",
        "åœ¨å‘é‡ç©ºé—´é‡Œï¼Œç›¸ä¼¼æ–‡æ¡£è·ç¦»æ›´è¿‘ã€‚\n",
        "\n",
        "ç”¨ K-means æŠŠè¿™äº›ç‚¹åˆ†æˆ K ç»„ï¼›æ¯ç»„ç”¨ä¸€ä¸ªè´¨å¿ƒä»£è¡¨ä¸»é¢˜ã€‚\n",
        "\n",
        "åšè¯­ä¹‰èšç±»æ—¶ï¼Œå¸¸ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ã€‚åšæ³•æ˜¯ï¼šæŠŠåµŒå…¥å•ä½åŒ–ï¼ˆL2=1ï¼‰ï¼Œè¿™æ ·ç”¨ K-means çš„æ¬§æ°è·ç¦»â‰ˆæœ€å¤§åŒ–ä½™å¼¦ç›¸ä¼¼åº¦ã€‚\n",
        "\n",
        "é€‰æ‹© Kï¼ˆå¤šå°‘ç°‡ï¼‰\n",
        "\n",
        "Elbowï¼ˆæ‰‹è‚˜æ³•ï¼‰ï¼šçœ‹ SSE éš K çš„ä¸‹é™æ‹ç‚¹ã€‚\n",
        "\n",
        "Silhouetteï¼šé€‰å¹³å‡è½®å»“ç³»æ•°æœ€é«˜çš„ Kã€‚\n",
        "\n",
        "å¦‚æœä¸æƒ³å…ˆå®š Kï¼Œè¯• HDBSCANï¼ˆå¯†åº¦èšç±»ï¼Œèƒ½ä¸¢å™ªå£°ç‚¹ï¼‰ã€‚"
      ],
      "metadata": {
        "id": "lJs5hqt5FCIq"
      },
      "id": "lJs5hqt5FCIq"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "95c055ce",
      "metadata": {
        "id": "95c055ce"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"The apple is a sweet fruit\",\n",
        "    \"Oranges are citrus fruits\",\n",
        "    \"Bananas are rich in potassium\",\n",
        "    \"Strawberries are red fruits\",\n",
        "    \"Dogs are domesticated animals\",\n",
        "    \"Cats are also pets\",\n",
        "    \"Elephants are the largest land mammals\",\n",
        "    \"Cows provide us with milk\",\n",
        "    \"Sharks are marine predators\",\n",
        "    \"Whales are the largest marine mammals\",\n",
        "    \"Dolphins are very intelligent\",\n",
        "    \"Artificial intelligence is the future\",\n",
        "    \"Machine learning is a subset of AI\",\n",
        "    \"Deep learning is a part of machine learning\",\n",
        "    \"Neural networks are used in deep learning\",\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({'documents': corpus})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "07529ddd",
      "metadata": {
        "id": "07529ddd"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode the documents in the corpus\n",
        "document_embeddings = model.encode(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ea806d9f",
      "metadata": {
        "id": "ea806d9f"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "num_clusters = 3\n",
        "clustering_model = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=300, n_init=10)\n",
        "clustering_model.fit(document_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "df['cluster'] = cluster_assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "fc24cc4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc24cc4f",
        "outputId": "e538ac52-7527-4d49-a2b3-4dbb34b3e5b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "['The apple is a sweet fruit' 'Oranges are citrus fruits'\n",
            " 'Bananas are rich in potassium' 'Strawberries are red fruits'] \n",
            "\n",
            "Cluster 1:\n",
            "['Dogs are domesticated animals' 'Cats are also pets'\n",
            " 'Elephants are the largest land mammals' 'Cows provide us with milk'\n",
            " 'Sharks are marine predators' 'Whales are the largest marine mammals'\n",
            " 'Dolphins are very intelligent'] \n",
            "\n",
            "Cluster 2:\n",
            "['Artificial intelligence is the future'\n",
            " 'Machine learning is a subset of AI'\n",
            " 'Deep learning is a part of machine learning'\n",
            " 'Neural networks are used in deep learning'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(df[df['cluster'] == i]['documents'].values, \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aadaa3a",
      "metadata": {
        "id": "9aadaa3a"
      },
      "source": [
        "# Community Detection with Sentence Transformers\n",
        "\n",
        "The sentence_transformers library provides a utility for community detection which applies a threshold on the cosine similarity score to identify distinct communities of sentences that are semantically similar. This method can be particularly helpful for organizing a large corpus of text into meaningful groups.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The [`community_detection`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.community_detection) function in the Sentence Transformers library is a useful utility for finding clusters or communities of semantically similar sentences. Here are the details of the function parameters:\n",
        "\n",
        "- `document_embeddings`: This is the list of embeddings for the documents in your corpus. The embeddings can be created using any of the Sentence Transformer models. The embeddings should be in the form of a 2D tensor or a list of 1D tensors. Each embedding should be a fixed-length vector that represents the semantic meaning of a document.\n",
        "\n",
        "- `threshold`: This is a float value between 0 and 1 that determines the cutoff for considering two documents to be part of the same community. It's based on the cosine similarity of the document embeddings. If the cosine similarity of two document embeddings is greater than the threshold, those two documents are considered to be in the same community. The higher the threshold, the more similar the documents in each community will be. However, a higher threshold may also result in more communities.\n",
        "\n",
        "- `min_community_size`: This is the minimum number of documents that a community must have. If a community has fewer than this number of documents, it will be discarded. The default value is 1, but you might want to set a higher value if you're interested in larger communities. This can help filter out noise and find more meaningful communities.\n",
        "\n",
        "- `batch_size`: As the function computes cosine similarities between document pairs, it may consume a significant amount of memory for a large corpus. To manage this, the computations are done in batches. The batch_size parameter determines the number of document pairs to compute similarities for in each batch. Larger batch sizes can be faster but consume more memory, while smaller batch sizes are slower but more memory-efficient.\n",
        "\n",
        "The function returns a list of communities, where each community is a list of indices in the original list of documents. Each community represents a group of semantically similar documents based on the provided threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç”¨ Sentence Transformers åšç¤¾ç¾¤/ç¤¾åŒºæ£€æµ‹\n",
        "\n",
        "\n",
        "sentence_transformers æä¾›äº†ä¸€ä¸ªâ€œç¤¾åŒºæ£€æµ‹â€å·¥å…·ï¼šåŸºäºä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ŒæŠŠè¯­ä¹‰ç›¸è¿‘çš„å¥å­åˆ’åˆ†åˆ°ä¸åŒçš„â€œç¤¾åŒºâ€ã€‚è¿™åœ¨æ•´ç†å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™æ—¶å¾ˆæœ‰ç”¨ã€‚\n",
        "community_detection å‡½æ•°å¯ç”¨äºå¯»æ‰¾è¯­ä¹‰ç›¸ä¼¼å¥å­çš„ç°‡/ç¤¾åŒºã€‚\n",
        "\n",
        "å‚æ•°è¯´æ˜ï¼š\n",
        "\n",
        "\n",
        "document_embeddingsï¼šè¯­æ–™ä¸­å„æ–‡æ¡£/å¥å­çš„åµŒå…¥åˆ—è¡¨ã€‚å¯ç”¨ä»»æ„ Sentence Transformer æ¨¡å‹å¾—åˆ°ã€‚åº”ä¸º 2D å¼ é‡æˆ– 1D å¼ é‡åˆ—è¡¨ï¼›æ¯ä¸ªåµŒå…¥æ˜¯è¡¨ç¤ºè¯­ä¹‰çš„å®šé•¿å‘é‡ã€‚\n",
        "\n",
        "\n",
        "thresholdï¼šå–å€¼ 0~1ï¼Œç”¨æ¥åˆ¤å®šä¸¤æ¡æ–‡æ¡£æ˜¯å¦åº”å½’ä¸ºåŒä¸€ç¤¾åŒºï¼Œä¾æ®æ˜¯å®ƒä»¬åµŒå…¥çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ç›¸ä¼¼åº¦é«˜äºè¯¥é˜ˆå€¼ â†’ è§†ä¸ºåŒä¸€ç¤¾åŒºã€‚é˜ˆå€¼è¶Šé«˜ï¼Œç¤¾åŒºå†…éƒ¨è¶Šç›¸ä¼¼ï¼Œä½†ç¤¾åŒºæ•°é‡å¯èƒ½å˜å¤šã€‚\n",
        "\n",
        "\n",
        "min_community_sizeï¼šç¤¾åŒºçš„æœ€å°æ–‡æ¡£æ•°ã€‚å°äºè¯¥æ•°é‡çš„ç¤¾åŒºä¼šè¢«ä¸¢å¼ƒã€‚é»˜è®¤ 1ï¼›è‹¥æƒ³å¾—åˆ°æ›´æœ‰æ„ä¹‰çš„ç¤¾åŒºï¼Œå¯è®¾å¤§ä¸€äº›ä»¥è¿‡æ»¤å™ªå£°ã€‚\n",
        "\n",
        "\n",
        "batch_sizeï¼šè®¡ç®—ä¸¤ä¸¤ä½™å¼¦ç›¸ä¼¼åº¦ä¼šå ç”¨è¾ƒå¤šå†…å­˜ï¼Œå› æ­¤æŒ‰æ‰¹è®¡ç®—ã€‚batch_size æ§åˆ¶æ¯æ‰¹è®¡ç®—çš„æ–‡æ¡£å¯¹æ•°é‡ã€‚æ‰¹è¶Šå¤§æ›´å¿«ä½†æ›´è€—å†…å­˜ï¼›æ‰¹å°æ›´çœå†…å­˜ä½†æ›´æ…¢ã€‚\n",
        "\n",
        "\n",
        "è¿”å›å€¼ï¼šä¸€ä¸ªç¤¾åŒºåˆ—è¡¨ï¼›æ¯ä¸ªç¤¾åŒºæ˜¯åŸå§‹æ–‡æ¡£åˆ—è¡¨ä¸­çš„ç´¢å¼•é›†åˆï¼Œè¡¨ç¤ºåœ¨ç»™å®šé˜ˆå€¼ä¸‹è¯­ä¹‰ç›¸ä¼¼çš„ä¸€ç»„æ–‡æ¡£ã€‚"
      ],
      "metadata": {
        "id": "sC7ocWhlF6c1"
      },
      "id": "sC7ocWhlF6c1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e3d1c9f",
      "metadata": {
        "id": "8e3d1c9f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.util import community_detection\n",
        "document_embeddings = model.encode(\n",
        "    corpus, show_progress_bar=True, convert_to_tensor=True\n",
        ")\n",
        "communities = community_detection(\n",
        "    document_embeddings, threshold=0.5, min_community_size=2, batch_size=1024\n",
        ")\n",
        "for i, comm in enumerate(communities):\n",
        "    print('_'*50)\n",
        "    print(f'community: {i}, size: {len(comm)}')\n",
        "    print('\\n'.join([corpus[ind] for ind in comm]))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b87688",
      "metadata": {
        "id": "e5b87688"
      },
      "source": [
        "In the output, we will see the communities of semantically similar sentences. Note that the choice of the threshold value can greatly affect the results: a lower threshold will result in larger but less cohesive communities, while a higher threshold will result in smaller but more tightly-knit communities.\n",
        "\n",
        "The community_detection function is a fast and efficient way to group similar sentences together, but keep in mind that it's a rather simple method based on thresholding the cosine similarity, and more sophisticated community detection methods might yield better results for certain tasks or datasets.\n",
        "\n",
        "This function is a great way to explore the semantic structure of your corpus and to get a high-level understanding of the main themes or topics in your text data."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "åœ¨è¾“å‡ºä¸­ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°è¯­ä¹‰ç›¸ä¼¼å¥å­å½¢æˆçš„ç¤¾åŒºã€‚æ³¨æ„ï¼šé˜ˆå€¼çš„é€‰æ‹©ä¼šæ˜æ˜¾å½±å“ç»“æœâ€”â€”é˜ˆå€¼ä½ï¼Œä¼šå¾—åˆ°æ›´å¤§ä½†å†…èšæ€§è¾ƒå¼±çš„ç¤¾åŒºï¼›é˜ˆå€¼é«˜ï¼Œä¼šå¾—åˆ°æ›´å°ä½†æ›´ç´§å¯†çš„ç¤¾åŒºã€‚\n",
        "\n",
        "community_detection å‡½æ•°èƒ½å¿«é€Ÿé«˜æ•ˆåœ°æŠŠç›¸ä¼¼å¥å­åˆ†åˆ°ä¸€èµ·ï¼Œä½†è¦è®°ä½å®ƒåªæ˜¯åŸºäºä½™å¼¦ç›¸ä¼¼åº¦é˜ˆå€¼çš„ç®€å•æ–¹æ³•ï¼›åœ¨æŸäº›ä»»åŠ¡æˆ–æ•°æ®é›†ä¸Šï¼Œæ›´å¤æ‚çš„ç¤¾åŒºæ£€æµ‹æ–¹æ³•å¯èƒ½æ•ˆæœæ›´å¥½ã€‚\n",
        "\n",
        "è¿™ä¸ªå‡½æ•°å¾ˆé€‚åˆæ¢ç´¢è¯­æ–™çš„è¯­ä¹‰ç»“æ„ï¼Œå¸®åŠ©ä½ ä»é«˜å±‚æ¬¡ç†è§£æ–‡æœ¬æ•°æ®é‡Œçš„ä¸»è¦ä¸»é¢˜æˆ–è¯é¢˜ã€‚"
      ],
      "metadata": {
        "id": "2inYHhwiGbbV"
      },
      "id": "2inYHhwiGbbV"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}